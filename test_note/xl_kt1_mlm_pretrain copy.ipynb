{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 보자!! 음 eos 토큰은 안쓰이는 숫자 여야 한다.\n",
    "\n",
    "\n",
    "concepts에 3~126이라 할때 \n",
    "response가 0,1이고 \n",
    "\n",
    "concepts도 eos가, response에도 eos가 있어야 하겠지?\n",
    "concepts+123*response = 이렇게 되면 기존에는 0~122 까지의 값에 0,123 이 더해지는 것이었잖아\n",
    "\n",
    "오답일 때 가질 수 있는 값은 0~122 \n",
    "정답일 떄 가질 수 있는 값은 123 ~225\n",
    "\n",
    "\n",
    "-----------------------------------------------------\n",
    "concepts을 3~125까지\n",
    "response를 0,1 이라 할때\n",
    "concepts+123*response =\n",
    "\n",
    "Interaction:\n",
    "오답일 때 가질 수 있는 값은 3~125 \n",
    "정답일 떄 가질 수 있는 값은 126 ~228\n",
    "\n",
    "Question:\n",
    "3~125\n",
    "\n",
    ";;;;;;;;;;;;\n",
    "eos =2 일때\n",
    "\n",
    "Interaction:\n",
    "오답일 때 가질 수 있는 값은 3~125 + eos 토큰끼리 더해졌을 때 4 추가 --> 곂침\n",
    "정답일 떄 가질 수 있는 값은 126 ~228 +eos 토큰끼리 더해졌을 때 4\n",
    "\n",
    "--> 이 경우 r에 더해진 eos값이 123이 곱해지며 또 달리 질 수 잇기에 r값에 eos를 더해주기 전에 미리 123(concept의)숫 자를 구한것을 따로 곱해두고 거기에 eos를 추가하자 \n",
    "r_shift경우 r을 두개로 만들어서 하나는 interaction용으로 빼두고 하나는 y값으로 쓰이게 eos만 추가해서 따로 두자!\n",
    "\n",
    "->거기다가 c,r에는 eos/2 를 더해 두개가 더해졌을 때 eos가 되도록 하고 \n",
    "c_shift, r_shift에는 온전한 eos를 더해두는 거지 그러면 문제 해결?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-17 13:46:15.114293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-17 13:46:15.698336: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.4/lib64:\n",
      "2024-01-17 13:46:15.698386: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.4/lib64:\n",
      "2024-01-17 13:46:15.698391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import copy\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "from preprocess.data_utils import read_data,get_max_concepts,calStatistics,extend_multi_concepts,id_mapping,save_id2idx,train_test_split,save_dcur,get_evalmask_token, get_mask_tokens\n",
    "import pickle\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from models.model_for_kt import TFTransfoXLModel,TFTransfoXLLMHeadModel,TFTransfoXLMLMHeadModel\n",
    "\n",
    "from transformers import TransfoXLConfig\n",
    "from tensorflow.keras.utils import register_keras_serializable\n",
    "import datetime\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config_xl = TransfoXLConfig(\n",
    "    data = '/home/jun/workspace/KT/data/ednet/data.txt',\n",
    "    dataset = 'wt103',\n",
    "    d_embed=128,\n",
    "    d_head = 32,\n",
    "    d_model=128,\n",
    "    mem_len=200,\n",
    "    n_head=8,\n",
    "    n_layer=6,\n",
    "    batch_size = 65,\n",
    "    tgt_len = 140,\n",
    "    ext_len = 0,\n",
    "    eval_tgt_len = 36,\n",
    "    eos_token=2,\n",
    "    num_c=123,\n",
    "    mask_token=3,\n",
    "    C_vocab_size=188,\n",
    "    Q_vocab_size = 12102,\n",
    "    R_vocab_size = 2,\n",
    "    epoch =1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete bad stu num of len: 7, delete interactions: 11, of r: 0, good num: 6162174\n",
      "====================\n",
      "original total interactions: 6162174, qs: 12102, cs: 188, seqnum: 4990\n",
      "df.columns: Index(['uid', 'is_repeat', 'concepts', 'usetimes', 'timestamps', 'responses',\n",
      "       'questions'],\n",
      "      dtype='object')\n",
      "====================\n",
      "after extend multi, total interactions: 15220330, qs: 12102, cs: 188, seqnum: 4990\n",
      "total num: 4990, train+valid num: 4491, test num: 499\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ALL_KEYS = [\"fold\", \"uid\", \"questions\", \"concepts\", \"responses\", \"timestamps\",\n",
    "#             \"usetimes\", \"selectmasks\", \"is_repeat\", \"qidxs\", \"rest\", \"orirow\", \"cidxs\"]\n",
    "# ONE_KEYS = [\"fold\", \"uid\"]\n",
    "\n",
    "total_df, effective_keys = read_data('/home/jun/workspace/KT/data/ednet/data.txt')\n",
    "\n",
    "stares = []\n",
    "\n",
    "if 'concepts' in effective_keys:\n",
    "    max_concepts = get_max_concepts(total_df)\n",
    "else:\n",
    "    max_concepts = -1\n",
    "\n",
    "oris, _, qs, cs, seqnum = calStatistics(total_df, stares, \"original\") # Check data status\n",
    "\n",
    "print(\"=\"*20)\n",
    "print(\n",
    "    f\"original total interactions: {oris}, qs: {qs}, cs: {cs}, seqnum: {seqnum}\")\n",
    "\n",
    "\n",
    "# questions ,concepts 값들의 숫자를 재정의 하여 0~ 나오도록 만들 면서 is_repeat값 처리\n",
    "# 14_54_14 으로 된 concepts를 14,54,14 로 분리하고 기존의 response도 똑같이 확장처리\n",
    "total_df_ex, effective_keys = extend_multi_concepts(total_df, effective_keys)\n",
    "total_df, dkeyid2idx = id_mapping(total_df_ex)\n",
    "dkeyid2idx[\"max_concepts\"] = max_concepts\n",
    "\n",
    "extends, _, qs, cs, seqnum = calStatistics(\n",
    "    total_df, stares, \"extend multi\")\n",
    "print(\"=\"*20)\n",
    "print(\n",
    "    f\"after extend multi, total interactions: {extends}, qs: {qs}, cs: {cs}, seqnum: {seqnum}\")\n",
    "\n",
    "\n",
    "#train test 분리\n",
    "train_df, test_df = train_test_split(total_df, 0.1)\n",
    "\n",
    "#transformer_xl dataset 만들기\n",
    "train = {\"qseqs\": [], \"cseqs\": [], \"masked_R\": [], \"labels\": []} #\"q_shift\": [], \"c_shift\": [], \"r_shift\": []}\n",
    "test = {\"qseqs\": [], \"cseqs\": [], \"masked_R\": [], \"labels\": []} #\"q_shift\": [], \"c_shift\": [], \"r_shift\": []}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-16 18:29:56.980363: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-16 18:29:56.980690: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-16 18:29:56.986097: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-16 18:29:56.986402: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-16 18:29:56.986690: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-16 18:29:56.986974: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-16 18:29:56.987760: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-16 18:29:57.170340: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-16 18:29:57.170660: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-16 18:29:57.170929: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-16 18:29:57.171184: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-16 18:29:57.171439: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-16 18:29:57.171692: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-16 18:29:57.735353: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-16 18:29:57.735680: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-16 18:29:57.735955: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-16 18:29:57.736304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-16 18:29:57.736560: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-16 18:29:57.736799: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6627 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-01-16 18:29:57.737048: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-16 18:29:57.737279: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 6627 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:02:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot convert 0.0 to EagerTensor of dtype int32",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_285233/2846199937.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mqes_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_xl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"qseqs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqes_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mrseq_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"responses\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mrseq_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_xl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mmasked_R\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_mask_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrseq_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconfig_xl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"masked_R\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasked_R\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/KT/preprocess/data_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(R, mask_token, mlm_probability)\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[0;31m# 특별 토큰에 대한 마스크 생성\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m     \u001b[0mspecial_token_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;31m# 확률 행렬에 특별 토큰 마스크 적용\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m     \u001b[0mprobability_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspecial_token_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobability_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;31m# 마스킹할 인덱스 생성\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[0mmasked_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mprobability_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/new/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/new/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(condition, t, e, name)\u001b[0m\n\u001b[1;32m   9402\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9403\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9404\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9405\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 9406\u001b[0;31m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   9407\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9408\u001b[0m       return select_v2_eager_fallback(\n\u001b[1;32m   9409\u001b[0m           condition, t, e, name=name, ctx=_ctx)\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot convert 0.0 to EagerTensor of dtype int32"
     ]
    }
   ],
   "source": [
    "\n",
    "# eos_token= 2 # config 파일로 조정할 수 있도록 수정 할 것!\n",
    "# num_c =123 # config 파일로 조정할 수 있도록 수정 할 것!\n",
    "for i, row in train_df.iterrows():\n",
    "#use kc_id or question_id as input\n",
    "\n",
    "    cseq_list=[int(_) for _ in row[\"concepts\"].split(\",\")]\n",
    "    cseq_list.append(config_xl.eos_token)\n",
    "    train[\"cseqs\"].append(cseq_list)\n",
    "\n",
    "    qes_list=[int(_) for _ in row[\"questions\"].split(\",\")]\n",
    "    qes_list.append(config_xl.eos_token)\n",
    "    train[\"qseqs\"].append(qes_list)\n",
    "\n",
    "\n",
    "    rseq_list=[(int(_)) for _ in row[\"responses\"].split(\",\")]\n",
    "    rseq_list.append(config_xl.eos_token)\n",
    "    masked_R, labels = get_mask_tokens(rseq_list,config_xl.mask_token)\n",
    "    train[\"masked_R\"].append(masked_R)\n",
    "    train[\"labels\"].append(labels)\n",
    "\n",
    "for i, row in test_df.iterrows():\n",
    "    #use kc_id or question_id as input\n",
    "    cseq_list=[int(_) for _ in row[\"concepts\"].split(\",\")]\n",
    "    cseq_list.append(config_xl.eos_token)   \n",
    "    test[\"cseqs\"].append(cseq_list)\n",
    "\n",
    "    qes_list=[int(_) for _ in row[\"questions\"].split(\",\")]\n",
    "    qes_list.append(config_xl.eos_token)\n",
    "    test[\"qseqs\"].append(qes_list)\n",
    "\n",
    "\n",
    "    rseq_list=[(int(_)) for _ in row[\"responses\"].split(\",\")]\n",
    "    rseq_list.append(config_xl.eos_token)\n",
    "    masked_R, labels = get_evalmask_token(rseq_list, config_xl.mask_token, config_xl.eos_token)\n",
    "    \n",
    "    test[\"masked_R\"].append(masked_R)\n",
    "    test[\"labels\"].append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_step 342\n",
      "342\n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "cseqs_list = list(itertools.chain(*train['cseqs']))\n",
    "qseqs_list = list(itertools.chain(*train['qseqs']))\n",
    "r_masked_list = tf.concat(train['masked_R'], axis=0)\n",
    "labels = tf.concat(train['labels'], axis=0)\n",
    "\n",
    "n_step = len(cseqs_list) // (config_xl.batch_size*config_xl.tgt_len)\n",
    "\n",
    "\n",
    "sliced_cseqs = tf.slice(cseqs_list,[0],[n_step * config_xl.batch_size*config_xl.tgt_len])  \n",
    "sliced_qseqs = tf.slice(qseqs_list,[0],[n_step * config_xl.batch_size*config_xl.tgt_len])  \n",
    "sliced_r_mask = tf.slice(r_masked_list,[0],[n_step * config_xl.batch_size*config_xl.tgt_len]) \n",
    "sliced_labels = tf.slice(labels,[0],[n_step * config_xl.batch_size*config_xl.tgt_len]) \n",
    "\n",
    "count =len(sliced_cseqs)// (config_xl.batch_size*config_xl.tgt_len)\n",
    "\n",
    "new_shape = (config_xl.batch_size, -1)  # 나머지 차원은 자동으로 계산됨\n",
    "\n",
    "cseq_reshaped = tf.reshape(sliced_cseqs, new_shape)\n",
    "qseq_reshaped = tf.reshape(sliced_qseqs, new_shape)\n",
    "r_mask_reshaped = tf.reshape(sliced_r_mask, new_shape)\n",
    "labels_reshaped = tf.reshape(sliced_labels, new_shape)\n",
    "\n",
    "cseq_transposed = tf.transpose(cseq_reshaped)\n",
    "qseq_transposed = tf.transpose(qseq_reshaped)\n",
    "r_mask_transposed = tf.transpose(r_mask_reshaped)\n",
    "labels_transposed = tf.transpose(labels_reshaped)\n",
    "\n",
    "\n",
    "#test\n",
    "test_cseqs_list = list(itertools.chain(*test['cseqs']))\n",
    "test_qseqs_list = list(itertools.chain(*test['qseqs']))\n",
    "test_r_masked_list = tf.concat(test['masked_R'], axis=0)\n",
    "test_labels = tf.concat(test['labels'], axis=0)\n",
    "\n",
    "\n",
    "\n",
    "# Work out how cleanly we can divide the dataset into bsz parts.\n",
    "# 아래의 두 코드는   data 텐서에서 배치 크기 bsz로 깔끔하게 맞지 않는 추가 요소를 제거하는 것 배치에 띡 떨어지게\n",
    "    \n",
    "test_n_step = len(test_cseqs_list) // (config_xl.batch_size*config_xl.tgt_len)\n",
    "print('n_step',test_n_step) # \n",
    "\n",
    "test_sliced_cseqs = tf.slice(test_cseqs_list,[0],[test_n_step * config_xl.batch_size*config_xl.tgt_len])  \n",
    "test_sliced_qseqs = tf.slice(test_qseqs_list,[0],[test_n_step * config_xl.batch_size*config_xl.tgt_len])  \n",
    "test_sliced_r_masked_seq = tf.slice(test_r_masked_list,[0],[test_n_step * config_xl.batch_size*config_xl.tgt_len]) \n",
    "\n",
    "test_sliced_labels = tf.slice(test_labels,[0],[test_n_step * config_xl.batch_size*config_xl.tgt_len])  \n",
    "\n",
    "count =len(test_sliced_cseqs)// (config_xl.batch_size*config_xl.tgt_len)\n",
    "print(count)\n",
    "\n",
    "'''# 시작 위치와 슬라이싱할 크기 설정\n",
    "begin = [0]  # 첫 번째 차원의 시작 위치는 0\n",
    "size = [6]   # 첫 번째 차원에서 6개의 원소를 슬라이싱\n",
    "\n",
    "# 데이터를 잘라내기 (tf.slice 사용)\n",
    "sliced_data = tf.slice(data, begin, size)  '''\n",
    "\n",
    "# Evenly divide the da\n",
    "# ta across the bsz batches.\n",
    "\n",
    "new_shape = (config_xl.batch_size, -1)  # 나머지 차원은 자동으로 계산됨\n",
    "\n",
    "test_qseq_reshaped = tf.reshape(test_sliced_qseqs, new_shape)\n",
    "test_cseq_reshaped = tf.reshape(test_sliced_cseqs, new_shape)\n",
    "test_r_masked_seq_reshaped = tf.reshape(test_sliced_r_masked_seq, new_shape)\n",
    "test_labels_reshaped = tf.reshape(test_sliced_labels, new_shape)\n",
    "\n",
    "\n",
    "test_qseq_reshaped = tf.transpose(test_qseq_reshaped)\n",
    "test_cseq_reshaped = tf.transpose(test_cseq_reshaped)\n",
    "test_r_masked_seq_reshaped = tf.transpose(test_r_masked_seq_reshaped)\n",
    "test_labels_reshaped = tf.cast(tf.transpose(test_labels_reshaped),tf.int64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (qseq_transposed, r_mask_transposed, labels_transposed))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (test_qseq_reshaped, test_r_masked_seq_reshaped, test_labels_reshaped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/jun/workspace/KT/xl_kt1_mlm_pretrain.ipynb 셀 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224a554e227d/home/jun/workspace/KT/xl_kt1_mlm_pretrain.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m train_dataset \u001b[39m=\u001b[39mtrain_dataset\u001b[39m.\u001b[39mbatch(config_xl\u001b[39m.\u001b[39mtgt_len)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224a554e227d/home/jun/workspace/KT/xl_kt1_mlm_pretrain.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m test_dataset \u001b[39m=\u001b[39mtest_dataset\u001b[39m.\u001b[39mbatch(config_xl\u001b[39m.\u001b[39mtgt_len)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataset =train_dataset.batch(config_xl.tgt_len)\n",
    "test_dataset =test_dataset.batch(config_xl.tgt_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = '/home/jun/workspace/KT/logs/gradient_tape/' + current_time +'{}ep_{}mem_Question/train'.format(config_xl.epoch, config_xl.mem_len)\n",
    "test_log_dir = '/home/jun/workspace/KT/logs/gradient_tape/' + current_time +'{}ep_{}mem_Question/test'.format(config_xl.epoch, config_xl.mem_len)\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_keras_serializable()\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = tf.cast(warmup_steps,tf.float32)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step =tf.cast(step, tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "    \n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'd_model': self.d_model,\n",
    "            'warmup_steps': self.warmup_steps\n",
    "            }\n",
    "learning_rate = CustomSchedule(config_xl.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFTransfoXLMLMHeadModel(config=config_xl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\n",
    "test_loss = tf.keras.metrics.Mean('test_loss', dtype=tf.float32)\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "      name='train_accuracy')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "  name='test_accuracy')\n",
    "test_precision = tf.metrics.Precision()\n",
    "test_recall = tf.metrics.Recall()\n",
    "train_auc = tf.keras.metrics.AUC()\n",
    "test_auc = tf.keras.metrics.AUC()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging. 모델그레프 그리기\n",
    "from datetime import datetime\n",
    "\n",
    "stamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "logdir = '/home/jun/workspace/KT/logs/func/%s' % stamp\n",
    "writer = tf.summary.create_file_writer(logdir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, data1,data2, target, mems, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = model(concepts=data1,responses=data2, labels=target, mems=mems)\n",
    "        loss = outputs.loss\n",
    "        mems = outputs.mems\n",
    "        loss_mx = target != -100\n",
    "        loss_value = loss[loss_mx]\n",
    "        loss_value = tf.reshape(loss_value, [-1, config_xl.R_vocab_size])\n",
    "        labels = target[loss_mx]\n",
    "\n",
    "        \n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=loss_value)\n",
    "        # batch_loss = tf.reduce_sum(loss) / valid_samples\n",
    "        mean_loss = tf.reduce_mean(loss)\n",
    "        train_loss(loss)\n",
    "        train_accuracy(labels,loss_value)\n",
    "        predictions =tf.nn.softmax(loss_value)\n",
    "        train_auc(tf.one_hot(labels, depth=predictions.shape[1]), predictions)\n",
    "\n",
    "    gradients = tape.gradient(mean_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    \n",
    "\n",
    "    return mems,mean_loss\n",
    "\n",
    "# def grad(model, inputs, targets):\n",
    "#   with tf.GradientTape() as tape:\n",
    "#     loss_value = loss(model, inputs, targets, training=True)\n",
    "#   return loss_value, tape.gradient(loss_value, model.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, mems, test_dataset):\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    evaluation_metrics = []\n",
    "    \n",
    "\n",
    "    for test_data1, test_data2, test_target in test_dataset:\n",
    "        outputs = model(concepts=test_data1, responses=test_data2, labels=test_target, mems=mems, training=False)\n",
    "        loss = outputs.loss\n",
    "        mems = outputs.mems\n",
    "\n",
    "        loss_mx = test_target != -100\n",
    "        loss_value = loss[loss_mx]\n",
    "        loss_value = tf.reshape(loss_value, [-1, config_xl.R_vocab_size])\n",
    "        labels = test_target[loss_mx]\n",
    "\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=loss_value)\n",
    "        mean_loss = tf.reduce_mean(loss)\n",
    "\n",
    "        # Update precision and recall metrics\n",
    "        predicted_labels = tf.argmax(loss_value, axis=1)\n",
    "        predictions =tf.nn.softmax(loss_value)\n",
    "\n",
    "        \n",
    "        test_auc(tf.one_hot(labels, depth=predictions.shape[1]), predictions)\n",
    "        test_precision(labels, predicted_labels)\n",
    "        test_recall(labels, predicted_labels)\n",
    "\n",
    "        test_accuracy(labels, loss_value)\n",
    "        test_loss(loss)\n",
    "        \n",
    "        \n",
    "        precision = test_precision.result().numpy()\n",
    "        recall = test_recall.result().numpy()\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall + 1e-7)\n",
    "\n",
    "        evaluation_metrics.append(test_accuracy.result().numpy())\n",
    "\n",
    "        total_loss += mean_loss.numpy()\n",
    "        num_batches += 1\n",
    "\n",
    "        \n",
    "        with test_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', test_loss.result(), step=num_batches)\n",
    "            tf.summary.scalar('accuracy', test_accuracy.result(), step=num_batches)\n",
    "            tf.summary.scalar('precision', test_precision.result(), step=num_batches)\n",
    "            tf.summary.scalar('recall', test_recall.result(), step=num_batches)\n",
    "            tf.summary.scalar('f1_score', f1_score, step=num_batches)\n",
    "            tf.summary.scalar('auc', test_auc.result(), step=num_batches)\n",
    "\n",
    "    # 평균 정밀도, 재현율, F1 점수를 계산합니다.\n",
    "    average_precision = test_precision.result().numpy()\n",
    "    average_recall = test_recall.result().numpy()\n",
    "    average_f1_score = 2 * (average_precision * average_recall) / (average_precision + average_recall + 1e-7)\n",
    "\n",
    "    average_metric = sum(evaluation_metrics) / len(evaluation_metrics)\n",
    "    average_loss = total_loss / num_batches\n",
    "\n",
    "    return average_loss, average_metric, average_precision, average_recall, average_f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-16 15:52:13.740674: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x564760618790 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-01-16 15:52:13.740694: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce RTX 2080 SUPER, Compute Capability 7.5\n",
      "2024-01-16 15:52:13.740698: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (1): NVIDIA GeForce RTX 2080 SUPER, Compute Capability 7.5\n",
      "2024-01-16 15:52:13.744377: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-01-16 15:52:13.823347: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 100 Loss 0.6370684504508972\n",
      "Epoch 1 Batch 200 Loss 0.58848637342453\n",
      "Epoch 1 Batch 300 Loss 0.5493931174278259\n",
      "Epoch 1 Batch 400 Loss 0.4001980125904083\n",
      "Epoch 1 Batch 500 Loss 0.31552058458328247\n",
      "Epoch 1 Batch 600 Loss 0.2752041518688202\n",
      "Epoch 1 Batch 700 Loss 0.2523006498813629\n",
      "Epoch 1 Batch 800 Loss 0.1527223289012909\n",
      "Epoch 1 Batch 900 Loss 0.15761373937129974\n",
      "Epoch 1 Batch 1000 Loss 0.14871670305728912\n",
      "Epoch 1 Batch 1100 Loss 0.14128609001636505\n",
      "Epoch 1 Batch 1200 Loss 0.13805948197841644\n",
      "Epoch 1 Batch 1300 Loss 0.11757903546094894\n",
      "Epoch 2 Batch 1400 Loss 0.15114034712314606\n",
      "Epoch 2 Batch 1500 Loss 0.12192797660827637\n",
      "Epoch 2 Batch 1600 Loss 0.16158121824264526\n",
      "Epoch 2 Batch 1700 Loss 0.11922312527894974\n",
      "Epoch 2 Batch 1800 Loss 0.1304556429386139\n",
      "Epoch 2 Batch 1900 Loss 0.10752145946025848\n",
      "Epoch 2 Batch 2000 Loss 0.13480199873447418\n",
      "Epoch 2 Batch 2100 Loss 0.12593580782413483\n",
      "Epoch 2 Batch 2200 Loss 0.12249918282032013\n",
      "Epoch 2 Batch 2300 Loss 0.12609970569610596\n",
      "Epoch 2 Batch 2400 Loss 0.10435594618320465\n",
      "Epoch 2 Batch 2500 Loss 0.1447896957397461\n",
      "Epoch 2 Batch 2600 Loss 0.13003495335578918\n",
      "Epoch 3 Batch 2700 Loss 0.14524078369140625\n",
      "Epoch 3 Batch 2800 Loss 0.1293879896402359\n",
      "Epoch 3 Batch 2900 Loss 0.13735558092594147\n",
      "Epoch 3 Batch 3000 Loss 0.13850504159927368\n",
      "Epoch 3 Batch 3100 Loss 0.13614298403263092\n",
      "Epoch 3 Batch 3200 Loss 0.13083431124687195\n",
      "Epoch 3 Batch 3300 Loss 0.1385653167963028\n",
      "Epoch 3 Batch 3400 Loss 0.09537877142429352\n",
      "Epoch 3 Batch 3500 Loss 0.08382736146450043\n",
      "Epoch 3 Batch 3600 Loss 0.1059400662779808\n",
      "Epoch 3 Batch 3700 Loss 0.14499828219413757\n",
      "Epoch 3 Batch 3800 Loss 0.1704002171754837\n",
      "Epoch 3 Batch 3900 Loss 0.13413479924201965\n",
      "Epoch 4 Batch 4000 Loss 0.10966943204402924\n",
      "Epoch 4 Batch 4100 Loss 0.13149672746658325\n",
      "Epoch 4 Batch 4200 Loss 0.10156330466270447\n",
      "Epoch 4 Batch 4300 Loss 0.13420183956623077\n",
      "Epoch 4 Batch 4400 Loss 0.1334967315196991\n",
      "Epoch 4 Batch 4500 Loss 0.08806618303060532\n",
      "Epoch 4 Batch 4600 Loss 0.09085819870233536\n",
      "Epoch 4 Batch 4700 Loss 0.13776440918445587\n",
      "Epoch 4 Batch 4800 Loss 0.09490376710891724\n",
      "Epoch 4 Batch 4900 Loss 0.13839372992515564\n",
      "Epoch 4 Batch 5000 Loss 0.12071935087442398\n",
      "Epoch 4 Batch 5100 Loss 0.10981733351945877\n",
      "Epoch 4 Batch 5200 Loss 0.10462220758199692\n",
      "Epoch 4 Batch 5300 Loss 0.09266866743564606\n",
      "Epoch 5 Batch 5400 Loss 0.11720871925354004\n",
      "Epoch 5 Batch 5500 Loss 0.10244318097829819\n",
      "Epoch 5 Batch 5600 Loss 0.1439426839351654\n",
      "Epoch 5 Batch 5700 Loss 0.09725980460643768\n",
      "Epoch 5 Batch 5800 Loss 0.08684925734996796\n",
      "Epoch 5 Batch 5900 Loss 0.10246050357818604\n",
      "Epoch 5 Batch 6000 Loss 0.10620591789484024\n",
      "Epoch 5 Batch 6100 Loss 0.10262429714202881\n",
      "Epoch 5 Batch 6200 Loss 0.11065001785755157\n",
      "Epoch 5 Batch 6300 Loss 0.08008040487766266\n",
      "Epoch 5 Batch 6400 Loss 0.1259174644947052\n",
      "Epoch 5 Batch 6500 Loss 0.10391735285520554\n",
      "Epoch 5 Batch 6600 Loss 0.13254821300506592\n"
     ]
    }
   ],
   "source": [
    "loss_values = []\n",
    "num_batches = 0\n",
    "# tf.summary.trace_on(graph=True, profiler=True)\n",
    "\n",
    "for epoch in range(config_xl.epoch):\n",
    "    start = time.time()\n",
    "    total_loss = 0.0\n",
    "    mems = None              # 첫 번째 모델의 메모리 상태\n",
    "           \n",
    "    \n",
    "    for data1, data2, target in test_dataset:\n",
    "    # for data1, data2, target in train_dataset:\n",
    "        mems, loss_value = train_step(model, data1,data2, target, mems, optimizer)\n",
    "        \n",
    "        num_batches += 1\n",
    "        total_loss += loss_value.numpy()\n",
    "        if num_batches % 100 == 0:\n",
    "            loss_values.append(loss_value.numpy())\n",
    "            print(f'Epoch {epoch + 1} Batch {num_batches} Loss {loss_value.numpy()}')\n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', train_loss.result(), step=num_batches)\n",
    "            tf.summary.scalar('accuracy', train_accuracy.result(), step=num_batches)\n",
    "            tf.summary.scalar('auc', train_auc.result(), step=num_batches)\n",
    "\n",
    "    # with writer.as_default():# 텐서보드에 모델 그래프 그리기\n",
    "    #     tf.summary.trace_export(\n",
    "    #         name=\"my_func_trace\",\n",
    "    #         step=0,\n",
    "    #         profiler_outdir=logdir)             \n",
    "\n",
    "    # Reset metrics every epoch\n",
    "    # train_loss.reset_states()\n",
    "    # test_loss.reset_states()\n",
    "    # train_accuracy.reset_states()\n",
    "    # test_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss on First Half Dataset after Epoch 5: 0.6319265374314715\n"
     ]
    }
   ],
   "source": [
    "# test_mems = None\n",
    "# test_loss0,test_acc0,average_precision, average_recall, average_f1_score = evaluate(model,test_mems, test_dataset)\n",
    "# print(f'Test Loss on First Half Dataset after Epoch {epoch + 1}: {test_loss0}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_weights('/home/jun/workspace/KT/save_model/5ep_600mem_questiion.ckpt/my_checkpoint') \n",
    "model.save('/home/jun/workspace/KT/save_model/5ep_600')\n",
    "\n",
    "# model.load_weights('/home/jun/workspace/KT/save_model/5ep_600mem.ckpt/my_checkpoint')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6319265374314715\n",
      "0.6484004206824721\n",
      "0.7520016045546908\n"
     ]
    }
   ],
   "source": [
    "print(test_loss0)\n",
    "print(test_acc0)\n",
    "print(average_f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###임베딩 데이터 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.plugins import projector\n",
    "\n",
    "log_dir='/home/jun/workspace/KT/logs/question/'\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Labels separately on a line-by-line manner.\n",
    "with open(os.path.join(log_dir, 'metadata.tsv'), \"w\") as f:\n",
    "  for concepts in dkeyid2idx['questions']:\n",
    "    f.write(\"{}\\n\".format(concepts))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = tf.Variable(model.transformer.word_emb_C.get_weights()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jun/workspace/KT/logs/question/embedding.ckpt-1'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = tf.train.Checkpoint(embedding=weights)\n",
    "checkpoint.save(os.path.join(log_dir, \"embedding.ckpt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up config.\n",
    "config = projector.ProjectorConfig()\n",
    "embedding = config.embeddings.add()\n",
    "# The name of the tensor will be suffixed by `/.ATTRIBUTES/VARIABLE_VALUE`.\n",
    "embedding.tensor_name = \"embedding/.ATTRIBUTES/VARIABLE_VALUE\"\n",
    "embedding.metadata_path = 'metadata.tsv'\n",
    "projector.visualize_embeddings(log_dir, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorboard --logdir /logs/imdb-example/  실행코드\n",
    "#체크포인트 저장하는 방법 공부해보자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non-CSV files in the folder: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Step 1: Get the folder path from the user\n",
    "folder_path = '/home/jun/workspace/KT/data/ednet/KT12/KT1/'\n",
    "\n",
    "# Step 2: Initialize a variable to store the count of files\n",
    "file_count = 0\n",
    "\n",
    "# Step 3: Iterate through the files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    # Check if the file is not a CSV file\n",
    "    if os.path.isfile(os.path.join(folder_path, filename)) and not filename.endswith('.csv'):\n",
    "        file_count += 1\n",
    "\n",
    "# Step 4: Print the number of non-CSV files in the folder\n",
    "print(f\"Number of non-CSV files in the folder: {file_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
