{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-04 10:53:20.642666: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-04 10:53:21.163604: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.4/lib64:\n",
      "2024-01-04 10:53:21.163647: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.4/lib64:\n",
      "2024-01-04 10:53:21.163652: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/jun/miniconda3/envs/new/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import copy\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "from data_utils import read_data,get_max_concepts,calStatistics,extend_multi_concepts,id_mapping,save_id2idx,train_test_split,save_dcur,generate_sequences, get_mask_tokens\n",
    "import pickle\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from model_for_kt import TFTransfoXLModel,TFTransfoXLLMHeadModel,TFTransfoXLMLMHeadModel\n",
    "\n",
    "from transformers import TransfoXLConfig\n",
    "from tensorflow.keras.utils import register_keras_serializable\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import horovod.tensorflow as hvd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-04 10:53:23.390687: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-04 10:53:23.391040: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-04 10:53:23.395658: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-04 10:53:23.395971: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-04 10:53:23.396326: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-04 10:53:23.396612: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-04 10:53:23.397632: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-04 10:53:23.570578: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-04 10:53:23.570902: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-04 10:53:23.571176: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-04 10:53:23.571436: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-04 10:53:23.571695: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-04 10:53:23.571954: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-04 10:53:24.160465: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-04 10:53:24.160802: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-04 10:53:24.161075: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-04 10:53:24.161335: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-04 10:53:24.161590: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-04 10:53:24.161834: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6627 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-01-04 10:53:24.162081: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-04 10:53:24.162313: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 6627 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:02:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "shape = [10, 4, 30]\n",
    "\n",
    "tensor = tf.random.uniform(shape)\n",
    "\n",
    "tt = tf.split(tensor, 3, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config_xl = TransfoXLConfig(\n",
    "    data = '/home/jun/workspace/KT/data/ednet/data.txt',\n",
    "    dataset = 'wt103',\n",
    "    d_embed=128,\n",
    "    d_head = 32,\n",
    "    d_model=128,\n",
    "    mem_len=400,\n",
    "    n_head=8,\n",
    "    n_layer=6,\n",
    "    batch_size = 10,\n",
    "    tgt_len = 18,\n",
    "    ext_len = 0,\n",
    "    eval_tgt_len = 36,\n",
    "    eos_token=2,\n",
    "    num_c=123,\n",
    "    mask_token=3,\n",
    "    C_vocab_size=123,\n",
    "    R_vocab_size = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATASET 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(data_path,bsz,bptt,eos_token,num_c,ext_len=None,):\n",
    "    \n",
    "    bsz = bsz#3 #60\n",
    "    bptt = bptt#36 #70\n",
    "    ext_len = ext_len if ext_len is not None else 0\n",
    "\n",
    "    # read txt dataset to dataframe\n",
    "    ALL_KEYS = [\"fold\", \"uid\", \"questions\", \"concepts\", \"responses\", \"timestamps\",\n",
    "                \"usetimes\", \"selectmasks\", \"is_repeat\", \"qidxs\", \"rest\", \"orirow\", \"cidxs\"]\n",
    "    ONE_KEYS = [\"fold\", \"uid\"]\n",
    "    \n",
    "    total_df, effective_keys = read_data(data_path)\n",
    "\n",
    "    stares = []\n",
    "\n",
    "    if 'concepts' in effective_keys:\n",
    "        max_concepts = get_max_concepts(total_df)\n",
    "    else:\n",
    "        max_concepts = -1\n",
    "\n",
    "    oris, _, qs, cs, seqnum = calStatistics(total_df, stares, \"original\")\n",
    "    print(\"=\"*20)\n",
    "    print(\n",
    "        f\"original total interactions: {oris}, qs: {qs}, cs: {cs}, seqnum: {seqnum}\")\n",
    "\n",
    "\n",
    "    # questions ,concepts 값들의 숫자를 재정의 하여 0~ 나오도록 만들 면서 is_repeat값 처리\n",
    "    total_df, effective_keys = extend_multi_concepts(total_df, effective_keys)\n",
    "    total_df, dkeyid2idx = id_mapping(total_df)\n",
    "    dkeyid2idx[\"max_concepts\"] = max_concepts\n",
    "\n",
    "    extends, _, qs, cs, seqnum = calStatistics(\n",
    "        total_df, stares, \"extend multi\")\n",
    "    print(\"=\"*20)\n",
    "    print(\n",
    "        f\"after extend multi, total interactions: {extends}, qs: {qs}, cs: {cs}, seqnum: {seqnum}\")\n",
    "\n",
    "\n",
    "    #train test 분리\n",
    "    train_df, test_df = train_test_split(total_df, 0.2)\n",
    "    \n",
    "\n",
    "    #transformer_xl dataset 만들기\n",
    "    dori = {\"qseqs\": [], \"cseqs\": [], \"rseqs\": []} #\"q_shift\": [], \"c_shift\": [], \"r_shift\": []}\n",
    "\n",
    "\n",
    "    # eos_token= 2 # config 파일로 조정할 수 있도록 수정 할 것!\n",
    "    # num_c =123 # config 파일로 조정할 수 있도록 수정 할 것!\n",
    "    for i, row in train_df.iterrows():\n",
    "    #use kc_id or question_id as input\n",
    "    \n",
    "        cseq_list=[int(_) for _ in row[\"concepts\"].split(\",\")]\n",
    "        cseq_list.append(eos_token)\n",
    "        dori[\"cseqs\"].append(cseq_list)\n",
    "\n",
    "        qes_list=[int(_) for _ in row[\"questions\"].split(\",\")]\n",
    "        qes_list.append(eos_token)\n",
    "        dori[\"qseqs\"].append(qes_list)\n",
    "\n",
    "\n",
    "        rseq_list=[(int(_)) for _ in row[\"responses\"].split(\",\")]\n",
    "        rseq_list.append(eos_token)\n",
    "        dori[\"rseqs\"].append(rseq_list)\n",
    "\n",
    "        # c_shift_list=[int(_) for _ in row[\"concepts\"].split(\",\")]\n",
    "        # c_shift_list.append(eos_token)\n",
    "        # dori[\"c_shift\"].append(c_shift_list)\n",
    "\n",
    "\n",
    "        # r_shift_list=[int(_) for _ in row[\"responses\"].split(\",\")]\n",
    "        # r_shift_list.append(eos_token)\n",
    "        # dori[\"r_shift\"].append(r_shift_list)\n",
    "\n",
    "    '''\n",
    "    딕셔너리의 각 값마다 끝에 eos 토큰 삽입\n",
    "    rseqs에는 num_c 곱하여 cseqs 더할 값 만들기\n",
    "\n",
    "    '''\n",
    "    cseqs_list = list(itertools.chain(*dori['cseqs']))\n",
    "    qseqs_list = list(itertools.chain(*dori['qseqs']))\n",
    "    rseqs_list = list(itertools.chain(*dori['rseqs']))\n",
    "    # c_shift_list = list(itertools.chain(*dori['c_shift']))\n",
    "    # r_shift_list = list(itertools.chain(*dori['r_shift']))\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    # 아래의 두 코드는   data 텐서에서 배치 크기 bsz로 깔끔하게 맞지 않는 추가 요소를 제거하는 것 배치에 띡 떨어지게\n",
    "    n_step = len(cseqs_list) // (bsz*bptt)\n",
    "    print('n_step',n_step) # \n",
    "    \n",
    "    sliced_cseqs = tf.slice(cseqs_list,[0],[n_step * bsz*bptt])  \n",
    "    sliced_qseqs = tf.slice(qseqs_list,[0],[n_step * bsz*bptt])  \n",
    "    sliced_rseqs = tf.slice(rseqs_list,[0],[n_step * bsz*bptt])  \n",
    "    # sliced_c_shift = tf.slice(c_shift_list,[0],[n_step * bsz*bptt])  \n",
    "    # sliced_r_shift = tf.slice(r_shift_list,[0],[n_step * bsz*bptt])  \n",
    "    # print('sliced_data',len(sliced_data))\n",
    "    # sliced_data = self.data[:self.n_step * self.bsz]\n",
    "    '''# 시작 위치와 슬라이싱할 크기 설정\n",
    "    begin = [0]  # 첫 번째 차원의 시작 위치는 0\n",
    "    size = [6]   # 첫 번째 차원에서 6개의 원소를 슬라이싱\n",
    "\n",
    "    # 데이터를 잘라내기 (tf.slice 사용)\n",
    "    sliced_data = tf.slice(data, begin, size)  '''\n",
    "\n",
    "    # Evenly divide the da\n",
    "    # ta across the bsz batches.\n",
    "\n",
    "    new_shape = (bsz, -1)  # 나머지 차원은 자동으로 계산됨\n",
    "\n",
    "    cseq_reshaped = tf.reshape(sliced_cseqs, new_shape)\n",
    "    rseq_reshaped = tf.reshape(sliced_rseqs, new_shape)\n",
    "    # c_shift_reshaped = tf.reshape(sliced_c_shift, new_shape)\n",
    "    # r_shift_reshaped = tf.reshape(sliced_r_shift, new_shape)\n",
    "    # data_transposed = tf.transpose(data_reshaped)\n",
    "    # print('interaction_reshaped',interaction_reshaped.shape)\n",
    "    split_num = 2 #GPU num\n",
    "\n",
    "\n",
    "    # first_half, second_half = tf.split(data, num_or_size_splits=split_num, axis=1)\n",
    "\n",
    "    n_batch = (n_step + bptt - 1) // bptt\n",
    "\n",
    "    for i in range(0, len(rseq_reshaped[1]) - 1, bptt):\n",
    "        \n",
    "        seq_len = min(bptt, rseq_reshaped.shape[1] - 1 - i) # # i값이 103227020를 넘지 않는 이상 seq_len = 70\n",
    "\n",
    "\n",
    "        end_idx = i + seq_len # 70,71,72,73,74......\n",
    "        beg_idx = max(0, i - ext_len) # 0,1,2,3,4,5\n",
    "        ''' 아래 처럼 첫번째 차원을 자르는 이류\n",
    "        로,또,1,등,당,첨 = > 로,또,1    => 로, 등\n",
    "                        등,당,첨         또, 당\n",
    "                                        1, 첨\n",
    "        '''\n",
    "\n",
    "        C = cseq_reshaped[:,beg_idx:end_idx] # self.data[:,0:70],[:,1:71] ~\n",
    "        R = rseq_reshaped[:,beg_idx:end_idx] # self.data[:,0:70],[:,1:71] ~\n",
    "        # label = rseq_reshaped[:,beg_idx:end_idx] # self.data[:,0:70],[:,1:71] ~\n",
    "        # Query = c_shift_reshaped[:,i+1:i+1+seq_len] # self.data[:,0:70],[:,1:71] ~\n",
    "        # r_shift = r_shift_reshaped[:,i+1:i+1+seq_len]\n",
    "\n",
    "        # second_half_data = second_half[:,beg_idx:end_idx] # self.data[:,0:70],[:,1:71] ~\n",
    "        # second_half_target = second_half[:,i+1:i+1+seq_len]\n",
    "        '''\n",
    "        여기서 원하는 값을 마스킹 해주도록 하자!\n",
    "        '''\n",
    "        masked_R, labels = get_mask_tokens(R,config_xl.mask_token)\n",
    "\n",
    "\n",
    "        if i + bptt < len(rseq_reshaped[1]) - 1:\n",
    "            yield C, masked_R,labels\n",
    "        # yield second_half_data, second_half_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_generator(\n",
    "     gen,\n",
    "     output_signature=(\n",
    "         tf.TensorSpec(shape=None, dtype=tf.int64),\n",
    "         tf.TensorSpec(shape=None, dtype=tf.int64),\n",
    "         tf.TensorSpec(shape=None, dtype=tf.int64),\n",
    "         ),\n",
    "     args=(config_xl.data,\n",
    "     config_xl.batch_size,\n",
    "     config_xl.tgt_len,\n",
    "     config_xl.eos_token,\n",
    "     config_xl.num_c)\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete bad stu num of len: 7, delete interactions: 11, of r: 0, good num: 6162174\n",
      "====================\n",
      "original total interactions: 6162174, qs: 12102, cs: 188, seqnum: 4990\n",
      "df.columns: Index(['uid', 'is_repeat', 'usetimes', 'responses', 'concepts', 'questions',\n",
      "       'timestamps'],\n",
      "      dtype='object')\n",
      "====================\n",
      "after extend multi, total interactions: 15220330, qs: 12102, cs: 188, seqnum: 4990\n",
      "total num: 4990, train+valid num: 3992, test num: 998\n",
      "n_step 67293\n",
      "(<tf.Tensor: shape=(10, 18), dtype=int64, numpy=\n",
      "array([[ 21,  35,  26,  10,   6,  92,  47,  43,   6,  90,  49,  47,  61,\n",
      "         62,  43,   8, 144,   6],\n",
      "       [176,  72, 183,  39, 176,  72, 183,  37, 176, 169,  63,  64,  30,\n",
      "         65,  37,  65, 169,  63],\n",
      "       [  8, 155,  46,  47,  48,  46,  30,  49,  45,  46,   8,  42,  41,\n",
      "         49, 156,  41,   8, 156],\n",
      "       [ 53,  43, 164,  77,  47, 164,  79,  47, 164,  81,  47,  82,  79,\n",
      "          9,  82,  77,   9,  82],\n",
      "       [ 57,  10, 117,  26,  28,  10,  27,  57,  26,  10,  16,  28,  28,\n",
      "         10,  27,  26,  57,  57],\n",
      "       [158,  41,  30,   9,  44,  41,   8, 155,  46,   8, 150,  41,  30,\n",
      "          9, 157,  41,  49, 156],\n",
      "       [ 25,  20,  26,  29, 143,  25,  26,  21,  29,  30,  28,  30,  29,\n",
      "         30,  10,  30,  96, 148],\n",
      "       [ 49, 152,  41,   9, 156,  41,   8,  44,  41,   8,  83,  81,   9,\n",
      "         83,  53,   9,  83,  52],\n",
      "       [ 29, 115,  26,  19,  31,  29,  16, 114,  27,  29,  26,  53,  47,\n",
      "          8,  79,  47,   8,  81],\n",
      "       [ 79,  49, 182,  30,  77,  49,  78,  30,  81,   8,  78,  30,  77,\n",
      "          8,  78,  30,  53,   8]])>, <tf.Tensor: shape=(10, 18), dtype=int64, numpy=\n",
      "array([[369, 369, 246, 246, 246, 246,   1, 246,   1, 369, 369, 369, 246,\n",
      "          1,   1, 246, 369,   1],\n",
      "       [369, 369, 369, 369, 369,   1, 246, 246, 246, 369, 369, 369, 369,\n",
      "        369,   1, 369, 369, 369],\n",
      "       [246, 369, 369, 369, 369,   1, 369,   1, 369, 369,   1, 369, 369,\n",
      "        369, 369, 369, 369, 369],\n",
      "       [246, 246, 369, 369, 369, 369, 369,   1, 246, 246, 246, 369, 369,\n",
      "          1, 246, 246, 246, 369],\n",
      "       [369, 369, 369,   1, 369, 246, 369,   1,   1, 369, 246, 369, 246,\n",
      "        246, 369,   1,   1, 246],\n",
      "       [369, 369, 369, 369, 369,   1, 369, 369, 369, 369, 369, 369, 369,\n",
      "        369,   1, 369, 369, 369],\n",
      "       [  1, 369, 246, 369, 369, 369, 369, 369, 369, 369, 369, 246, 246,\n",
      "        246, 246, 369, 369, 246],\n",
      "       [246, 369, 369, 369,   1, 369,   1, 369, 369, 369, 246, 246, 246,\n",
      "        369, 369, 369, 246, 246],\n",
      "       [369, 369, 369, 369,   1, 369, 369, 369, 369, 246, 246, 369, 369,\n",
      "        369,   1, 369, 369, 369],\n",
      "       [246, 246, 369,   1, 369, 369,   1, 369, 369, 369, 369, 369, 369,\n",
      "          1, 369, 369, 369, 369]])>, <tf.Tensor: shape=(10, 18), dtype=int64, numpy=\n",
      "array([[-100, -100, -100, -100, -100, -100,    0, -100,    1, -100, -100,\n",
      "        -100, -100,    0,    0, -100, -100,    1],\n",
      "       [-100, -100,    1, -100, -100,    0, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100,    1, -100, -100, -100],\n",
      "       [-100, -100, -100, -100, -100,    1, -100,    1, -100, -100,    1,\n",
      "        -100, -100, -100, -100, -100, -100, -100],\n",
      "       [-100, -100, -100, -100,    1, -100,    1,    1, -100, -100, -100,\n",
      "        -100, -100,    1, -100,    0, -100, -100],\n",
      "       [-100, -100, -100,    0, -100, -100, -100,    1,    0, -100,    0,\n",
      "        -100, -100, -100, -100,    0,    1,    0],\n",
      "       [-100, -100, -100, -100, -100,    1, -100, -100, -100, -100, -100,\n",
      "        -100,    1, -100,    1, -100, -100, -100],\n",
      "       [   1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100],\n",
      "       [-100, -100, -100, -100,    1,    1,    1, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100],\n",
      "       [-100,    1, -100, -100,    1, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100,    1, -100, -100, -100],\n",
      "       [-100, -100, -100,    1, -100, -100,    1, -100, -100, -100,    1,\n",
      "        -100, -100,    1, -100, -100,    1, -100]])>)\n"
     ]
    }
   ],
   "source": [
    "A=next(iter(dataset))\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "con, r_ma, label=A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 10), dtype=int64, numpy=\n",
       "array([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
       "       [-100, -100, -100, -100, -100, -100, -100,    1, -100, -100],\n",
       "       [-100, -100, -100, -100, -100, -100, -100, -100, -100,    1]])>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int64, numpy=array([1, 1])>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_mx = label != -100\n",
    "labels = label[loss_mx]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete bad stu num of len: 279, delete interactions: 442, of r: 0, good num: 337559\n",
      "====================\n",
      "original total interactions: 337559, qs: 17737, cs: 123, seqnum: 3884\n",
      "df.columns: Index(['uid', 'is_repeat', 'responses', 'concepts', 'questions'], dtype='object')\n",
      "====================\n",
      "after extend multi, total interactions: 337559, qs: 17737, cs: 123, seqnum: 3884\n",
      "total num: 3884, train+valid num: 3108, test num: 776\n",
      "n_step 9259\n",
      "9258\n"
     ]
    }
   ],
   "source": [
    "\n",
    "count =0 \n",
    "for sample in dataset:\n",
    "    count += 1\n",
    "print(count)\n",
    "\n",
    "first_half_dataset = dataset.take(count//2)\n",
    "second_half_dataset = dataset.skip((count // 2) + (count % 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_masked_language_xl_model(xl_model):\n",
    "    \n",
    "#     # Input layer\n",
    "#     inputs = tf.keras.layers.Input((config_xl.MAX_LEN,), dtype=tf.int64)\n",
    "\n",
    "#     # Word embedding layer\n",
    "#     word_embeddings = tf.keras.layers.Embedding(config_xl.VOCAB_SIZE, config_xl.EMBED_DIM, name=\"word_embedding\")(inputs)\n",
    "    \n",
    "#     # Position embedding layer\n",
    "#     position_embeddings = tf.keras.layers.Embedding(input_dim=config_xl.MAX_LEN,\n",
    "#                                                     output_dim=vars.EMBED_DIM,\n",
    "#                                                     weights=[get_pos_encoding_matrix(config_xl.MAX_LEN, config_xl.EMBED_DIM)],\n",
    "#                                                     name=\"position_embedding\",\n",
    "#                                                     )(tf.range(start=0, limit=config_xl.MAX_LEN, delta=1))\n",
    "    \n",
    "#     # Add the word and position embeddings\n",
    "#     embeddings = word_embeddings + position_embeddings\n",
    "\n",
    "#     # Get the output of the embedding layer\n",
    "#     encoder_output = embeddings\n",
    "    \n",
    "#     # Loop over the number of layers\n",
    "#     for i in range(config.NUM_LAYERS):\n",
    "        \n",
    "#         # Create BERT module\n",
    "#         encoder_output = xl_model(encoder_output, encoder_output, encoder_output, i)\n",
    "\n",
    "#     # Output layer\n",
    "#     mlm_output = tf.keras.layers.Dense(config.VOCAB_SIZE, name=\"mlm_cls\", activation=\"softmax\")(encoder_output)\n",
    "    \n",
    "#     # MLM model\n",
    "#     mlm_model = MaskedLanguageModel(inputs, mlm_output, name=\"masked_bert_model\")\n",
    "\n",
    "#     # Adam optimizer\n",
    "#     optimizer = tf.keras.optimizers.Adam(learning_rate=config.LR)\n",
    "    \n",
    "#     # Compile the model\n",
    "#     mlm_model.compile(optimizer=optimizer)\n",
    "    \n",
    "#     return mlm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@register_keras_serializable()\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = tf.cast(warmup_steps,tf.float32)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step =tf.cast(step, tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "    \n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)*hvd.size()\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'd_model': self.d_model,\n",
    "            'warmup_steps': self.warmup_steps\n",
    "            }\n",
    "learning_rate = CustomSchedule(config_xl.d_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "해야할 것 정리\n",
    "\n",
    "\n",
    "모델 모아져 있는 곳 같서 새로운 클래스 하나 만들고 거기에다가\n",
    "https://github.com/soheil-mp/BERT/blob/main/BERT-MLM.ipynb\n",
    "여기서 나오는 부분 확인하고 끝부분 코드랑 loss 구하는거 갓다 밖아서 lml모델 되게 만들기가 먼저다 내일!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CustomSchedule 및 모델 정의는 이전과 동일하게 유지합니다.\n",
    "# ...\n",
    "hvd.init()\n",
    "\n",
    "# 옵티마이저 정의 및 Horovod 래핑\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "optimizer = hvd.DistributedOptimizer(optimizer)\n",
    "\n",
    "# 모델 정의\n",
    "\n",
    "# 모델 및 데이터셋 생성\n",
    "if hvd.rank() == 0:\n",
    "    model0 = TFTransfoXLMLMHeadModel(config=config_xl)\n",
    "  # GPU:0에서 사용할 첫 번째 모델\n",
    "    dataset0 = first_half_dataset       # 첫 번째 데이터셋\n",
    "    mems0 = None              # 첫 번째 모델의 메모리 상태\n",
    "elif hvd.rank() == 1:\n",
    "    model1 = TFTransfoXLMLMHeadModel(config=config_xl)\n",
    "  # GPU:1에서 사용할 두 번째 모델\n",
    "    dataset1 = second_half_dataset       # 두 번째 데이터셋\n",
    "    mems1 = None              # 두 번째 모델의 메모리 상태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "# 훈련 스텝 정의\n",
    "@tf.function\n",
    "def train_step(model, data1,data2, target, mems, optimizer,first_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = model(concepts=data1,responses=data2, labels=target, mems=mems)\n",
    "        loss = outputs.loss\n",
    "        mems = outputs.mems\n",
    "\n",
    "        loss_mx = target != -100\n",
    "        loss_value = loss[loss_mx]\n",
    "        loss_value = tf.reshape(loss_value, [-1, config_xl.R_vocab_size])\n",
    "        labels = target[loss_mx]\n",
    "        label = tf.reshape(labels, [-1])    \n",
    "\n",
    "        # valid_samples = tf.reduce_sum(tf.cast(loss_mx, tf.float32))\n",
    "\n",
    "        # tf.print('loss_mx',loss_mx)\n",
    "        # tf.print('valid_samples',valid_samples)\n",
    "\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=label, logits=loss_value)\n",
    "\n",
    "        # batch_loss = tf.reduce_sum(loss) / valid_samples\n",
    "        mean_loss = tf.reduce_mean(loss)\n",
    "\n",
    "    # Horovod: add Horovod Distributed GradientTape.\n",
    "    tape = hvd.DistributedGradientTape(tape)\n",
    "\n",
    "    gradients = tape.gradient(mean_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    if first_batch:\n",
    "        hvd.broadcast_variables(model.variables, root_rank=0)\n",
    "        hvd.broadcast_variables(optimizer.variables(), root_rank=0)\n",
    "\n",
    "    return mems,mean_loss\n",
    "\n",
    "\n",
    "    return mems, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete bad stu num of len: 279, delete interactions: 442, of r: 0, good num: 337559\n",
      "====================\n",
      "original total interactions: 337559, qs: 17737, cs: 123, seqnum: 3884\n",
      "df.columns: Index(['uid', 'is_repeat', 'responses', 'concepts', 'questions'], dtype='object')\n",
      "====================\n",
      "after extend multi, total interactions: 337559, qs: 17737, cs: 123, seqnum: 3884\n",
      "total num: 3884, train+valid num: 3108, test num: 776\n",
      "n_step 9259\n",
      "Epoch 1 Batch 100 Loss 0.782087504863739\n",
      "Epoch 1 Batch 200 Loss 0.6394003629684448\n",
      "Epoch 1 Batch 300 Loss 0.44817978143692017\n",
      "Epoch 1 Batch 400 Loss 0.8918962478637695\n",
      "Epoch 1 Batch 500 Loss 0.5986754298210144\n",
      "Epoch 1 Batch 600 Loss 0.7245195508003235\n",
      "Epoch 1 Batch 700 Loss 0.6492756605148315\n",
      "Epoch 1 Batch 800 Loss 0.33166036009788513\n",
      "Epoch 1 Batch 900 Loss 0.6384669542312622\n",
      "Epoch 1 Batch 1000 Loss 0.6760245561599731\n",
      "Epoch 1 Batch 1100 Loss 0.7209464907646179\n",
      "Epoch 1 Batch 1200 Loss 0.7505761981010437\n",
      "Epoch 1 Batch 1300 Loss 0.5977612733840942\n",
      "Epoch 1 Batch 1400 Loss 0.567508339881897\n",
      "Epoch 1 Batch 1500 Loss 0.8457213640213013\n",
      "Epoch 1 Batch 1600 Loss 0.6437013149261475\n",
      "Epoch 1 Batch 1700 Loss 0.761122465133667\n",
      "Epoch 1 Batch 1800 Loss 1.149018406867981\n",
      "Epoch 1 Batch 1900 Loss 0.6617183685302734\n",
      "Epoch 1 Batch 2000 Loss 0.6369498372077942\n",
      "Epoch 1 Batch 2100 Loss 0.4988519847393036\n",
      "Epoch 1 Batch 2200 Loss 0.5707507729530334\n",
      "Epoch 1 Batch 2300 Loss 0.344352126121521\n",
      "Epoch 1 Batch 2400 Loss 0.6847773790359497\n",
      "Epoch 1 Batch 2500 Loss 0.5855706930160522\n",
      "Epoch 1 Batch 2600 Loss 0.8093219995498657\n",
      "Epoch 1 Batch 2700 Loss 0.6770676374435425\n",
      "Epoch 1 Batch 2800 Loss 0.785586416721344\n",
      "Epoch 1 Batch 2900 Loss 0.7566048502922058\n",
      "Epoch 1 Batch 3000 Loss 0.5379837155342102\n",
      "Epoch 1 Batch 3100 Loss 0.5558291077613831\n",
      "Epoch 1 Batch 3200 Loss 0.5661133527755737\n",
      "Epoch 1 Batch 3300 Loss 0.40313276648521423\n",
      "Epoch 1 Batch 3400 Loss 1.3249205350875854\n",
      "Epoch 1 Batch 3500 Loss 0.4702532887458801\n",
      "Epoch 1 Batch 3600 Loss 0.7741988897323608\n",
      "Epoch 1 Batch 3700 Loss 0.5784493684768677\n",
      "Epoch 1 Batch 3800 Loss 0.6776570081710815\n",
      "Epoch 1 Batch 3900 Loss 0.35392263531684875\n",
      "Epoch 1 Batch 4000 Loss 0.2623171806335449\n",
      "Epoch 1 Batch 4100 Loss 0.677849531173706\n",
      "Epoch 1 Batch 4200 Loss 0.8299916982650757\n",
      "Epoch 1 Batch 4300 Loss 0.8669322729110718\n",
      "Epoch 1 Batch 4400 Loss 0.6270875930786133\n",
      "Epoch 1 Batch 4500 Loss 0.6393488049507141\n",
      "Epoch 1 Batch 4600 Loss 0.502432107925415\n",
      "delete bad stu num of len: 279, delete interactions: 442, of r: 0, good num: 337559\n",
      "====================\n",
      "original total interactions: 337559, qs: 17737, cs: 123, seqnum: 3884\n",
      "df.columns: Index(['uid', 'is_repeat', 'responses', 'concepts', 'questions'], dtype='object')\n",
      "====================\n",
      "after extend multi, total interactions: 337559, qs: 17737, cs: 123, seqnum: 3884\n",
      "total num: 3884, train+valid num: 3108, test num: 776\n",
      "n_step 9259\n",
      "Epoch 2 Batch 100 Loss 0.5318543910980225\n",
      "Epoch 2 Batch 200 Loss 0.9202297925949097\n",
      "Epoch 2 Batch 300 Loss 0.38165563344955444\n",
      "Epoch 2 Batch 400 Loss 0.940825343132019\n",
      "Epoch 2 Batch 500 Loss 0.5662451982498169\n",
      "Epoch 2 Batch 600 Loss 0.5568643808364868\n",
      "Epoch 2 Batch 700 Loss 0.8215324282646179\n",
      "Epoch 2 Batch 800 Loss 0.5631643533706665\n",
      "Epoch 2 Batch 900 Loss 0.5204408168792725\n",
      "Epoch 2 Batch 1000 Loss 0.6880977749824524\n",
      "Epoch 2 Batch 1100 Loss 0.5864914655685425\n",
      "Epoch 2 Batch 1200 Loss 0.36289167404174805\n",
      "Epoch 2 Batch 1300 Loss 0.9761143922805786\n",
      "Epoch 2 Batch 1400 Loss 0.44263944029808044\n",
      "Epoch 2 Batch 1500 Loss 0.6831415295600891\n",
      "Epoch 2 Batch 1600 Loss 0.7359545826911926\n",
      "Epoch 2 Batch 1700 Loss 0.7333800196647644\n",
      "Epoch 2 Batch 1800 Loss 0.6408803462982178\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/jun/workspace/KT/transformer_Xl.ipynb 셀 19\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B119.69.75.176/home/jun/workspace/KT/transformer_Xl.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mif\u001b[39;00m hvd\u001b[39m.\u001b[39mrank() \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B119.69.75.176/home/jun/workspace/KT/transformer_Xl.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mfor\u001b[39;00m data1, data2, target \u001b[39min\u001b[39;00m dataset0:\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B119.69.75.176/home/jun/workspace/KT/transformer_Xl.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m         mems0, loss_value \u001b[39m=\u001b[39m train_step(model0, data1,data2, target, mems0, optimizer,num_batches\u001b[39m==\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B119.69.75.176/home/jun/workspace/KT/transformer_Xl.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m         \u001b[39m# print('loss_value',loss_value)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B119.69.75.176/home/jun/workspace/KT/transformer_Xl.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m         num_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/new/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/new/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda3/envs/new/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:919\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    917\u001b[0m \u001b[39m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    918\u001b[0m \u001b[39m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 919\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    920\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[1;32m    921\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCreating variables on a non-first call to a function\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    922\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39m decorated with tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/new/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[1;32m    133\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    135\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/new/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1741\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1742\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1746\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/envs/new/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    377\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    379\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    380\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    381\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    382\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    383\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    384\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    386\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    387\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    391\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/miniconda3/envs/new/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for epoch in range(2):\n",
    "    start = time.time()\n",
    "    num_batches = 0\n",
    "    total_loss = 0.0\n",
    "    if hvd.rank() == 0:\n",
    "        for data1, data2, target in dataset0:\n",
    "            mems0, loss_value = train_step(model0, data1,data2, target, mems0, optimizer,num_batches==0)\n",
    "            # print('loss_value',loss_value)\n",
    "            num_batches += 1\n",
    "            total_loss += loss_value.numpy()\n",
    "            if num_batches % 100 == 0:\n",
    "                print(f'Epoch {epoch + 1} Batch {num_batches} Loss {loss_value.numpy()}')\n",
    "    elif hvd.rank() == 1:\n",
    "        for data1,data2, target in dataset1:\n",
    "            mems1, loss_value = train_step(model1, data1,data2, target, mems1, optimizer,num_batches==0)\n",
    "            num_batches += 1\n",
    "            total_loss += loss_value.numpy()\n",
    "            if num_batches % 100 == 0:\n",
    "                print(f'Epoch {epoch + 1} Batch {num_batches} Loss {loss_value.numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(9, 2), dtype=float32, numpy=\n",
       "array([[-79036.91 , -79078.23 ],\n",
       "       [-79036.914, -79078.23 ],\n",
       "       [-79036.914, -79078.23 ],\n",
       "       [-79036.91 , -79078.22 ],\n",
       "       [-79036.914, -79078.234],\n",
       "       [-79036.914, -79078.23 ],\n",
       "       [-79036.91 , -79078.22 ],\n",
       "       [-79036.91 , -79078.23 ],\n",
       "       [-79036.91 , -79078.23 ]], dtype=float32)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels\n",
    "loss_mx\n",
    "\n",
    "labels[loss_mx]\n",
    "loss_value[loss_mx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 10), dtype=int64, numpy=\n",
       "array([[-100, -100, -100, -100, -100, -100, -100, -100, -100,    1],\n",
       "       [-100,    1, -100,    1, -100, -100, -100, -100, -100, -100],\n",
       "       [-100, -100, -100, -100,    1, -100, -100, -100, -100,    1]])>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masked_language_bert_model():\n",
    "    \n",
    "    # Input layer\n",
    "    inputs = tf.keras.layers.Input((config.MAX_LEN,), dtype=tf.int64)\n",
    "\n",
    "    # Word embedding layer\n",
    "    word_embeddings = tf.keras.layers.Embedding(config.VOCAB_SIZE, config.EMBED_DIM, name=\"word_embedding\")(inputs)\n",
    "    \n",
    "    # Position embedding layer\n",
    "    position_embeddings = tf.keras.layers.Embedding(input_dim=config.MAX_LEN,\n",
    "                                                    output_dim=config.EMBED_DIM,\n",
    "                                                    weights=[get_pos_encoding_matrix(config.MAX_LEN, config.EMBED_DIM)],\n",
    "                                                    name=\"position_embedding\",\n",
    "                                                    )(tf.range(start=0, limit=config.MAX_LEN, delta=1))\n",
    "    \n",
    "    # Add the word and position embeddings\n",
    "    embeddings = word_embeddings + position_embeddings\n",
    "\n",
    "    # Get the output of the embedding layer\n",
    "    encoder_output = embeddings\n",
    "    \n",
    "    # Loop over the number of layers\n",
    "    for i in range(config.NUM_LAYERS):\n",
    "        \n",
    "        # Create BERT module\n",
    "        encoder_output = bert_module(encoder_output, encoder_output, encoder_output, i)\n",
    "\n",
    "    # Output layer\n",
    "    mlm_output = tf.keras.layers.Dense(config.VOCAB_SIZE, name=\"mlm_cls\", activation=\"softmax\")(encoder_output)\n",
    "    \n",
    "    # MLM model\n",
    "    mlm_model = MaskedLanguageModel(inputs, mlm_output, name=\"masked_bert_model\")\n",
    "\n",
    "    # Adam optimizer\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=config.LR)\n",
    "    \n",
    "    # Compile the model\n",
    "    mlm_model.compile(optimizer=optimizer)\n",
    "    \n",
    "    return mlm_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
